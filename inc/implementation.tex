\chapter{Implementation}
\label{chap:implementation}
% - This has the description of how you actually went about implementing the project.  This should be focused on the interesting challenges and how those related to the project.

The software used for measuring the detection thresholds was made in unreal engine 4 version 4.18.3. The virtual environment in which the experiments were tested was built to look like a skyscraper with each of the different test taking place in different rooms, as seen in figure~\ref{fig:game_room}. All the assets used were either from epic games blueprint tutorial, unreal engine's starter content, VR package, first-person shooter package or made by the author. Each of the floors was decorated sufficiently to provide enough visual density \cite{paludan2016disguising} for redirected walking to be noticeable.

The implementation of redirected walking for this paper took inspiration from the redirected walking toolkit \cite{azmandian2016redirected}. However, given the different game engines used the final product became quite different. The object used for implementing the gains in this paper featured a root component, a camera component, two motion controller components with a controller static mesh subcomponent for each of the motion controller components. As the camera component is controlled through the HMD it's relative position and rotation can be used to monitor the movement of the wearer. To be able to apply the different gains the delta rotation and delta movement of the camera component was calculated at each tick. The delta rotation is calculated as the signed angle changes along the z-axis (yaw), while the delta movement is calculated as the difference between current and previous position but only along the x and y-axis. To avoid affecting the relationship between the relative position of the camera and the real position of the VR headset the gains were applied to the whole object instead of the camera component.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/InGameView.png}
    \caption{VR room used for translation and curvature in experiment 2}
    \label{fig:game_room}
\end{figure}

\subsection{Rotation gain}
The initial step for applying rotation gain is calculating how much the virtual rotation of the actor needs to change ($\Delta R_{virtual}$) to apply given the changes in the real rotation ($\Delta R_{real}$) and the rotation gain ($G_{r}$)
\[\Delta R_{virtual} = \Delta R_{real} * G_{r}\]
For rotation gain, it was required to smooth the rotation as low rotation gain values caused jittering. To calculate the smoothed rotation ($sR_{virtual}$) the last rotation ($lR_{virtual}$) applied was used along with a smoothing factor ($S_f$) of 0.5.
\[\Delta sR_{virtual} = (R_{virtual} * S_f) + (lR_{virtual} * (1 - S_f)) \]
Having found the rotation the next step is to find how much the object needs to be moved ($\Delta Pos $) so that the rotation applied to the object will cause the camera object to be rotated while staying in the same position. To find this position we first get the vector ($V_{CO}$) from the camera (C) to the object (O).
\[V_{CO} = O_{pos} - C_{pos} \]
Then the vector is rotated around the z-axis. We then subtract the rotated vector by the vector from the camera to object again as this will give us the vector for how much the actor is required to move, as the rotated vector only gives how much the camera would need to move to reach the same position.
\[\Delta Pos = RotateAroundAxis( V_{CO}, sR_{virtual}, Z_{axis}) -  V_{CO})\]
The $\Delta Pos$ and $\Delta sR_{virtual}$ are then added to the objects world transform. 

\subsection{Translation gain}
Before applying the translation gain ($G_{t}$) the changes in real movement ($\Delta Mov_{real}$) needed to be rotated to match the objects rotation. If not the translation gain would not be applied in the right direction while the objects rotation was not 0 around the z axis.
\[r\Delta Mov = RotateAroundAxis(\Delta Mov_{real}, O_{rot.z}, Z_{axis}) \]
Then how much the object should be moved can be calculated.
\[\Delta Mov = Normalize(r\Delta Mov) * ( |r\Delta Mov| * G_{t})\]
The $\Delta Mov$ is then added to the actors world offset.

\subsection{Curvature gain}
Curvature gain ($G_{c}$) is implemented similarly to rotation gain with the exception being how the change in virtual rotation ($\Delta R_{virtual}$) is calculated.
\[\Delta R_{virtual} = (\frac{|\Delta Mov_{real}|}{G_{c}}) * 360\]
Then as with rotation gain, the same math is used to calculate what position that needs objects to be moved to for the rotation to apply to the camera but not move it. The rotation and distance are then added to the objects world transform. In order to avoid dividing by 0 or any other errors related to 0 values if statements checked that the gain, delta movement or delta rotation not 0 before calculated and applying gains.